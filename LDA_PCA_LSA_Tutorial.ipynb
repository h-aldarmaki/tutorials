{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM51cP1wm9Elog8T4SE6vXv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h-aldarmaki/tutorials/blob/main/LDA_PCA_LSA_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial was prepared for lecture 7 in **AI701: Foundations of Atificial Initellignece** at [MBZUAI](https://mbzuai.ac.ae). \n",
        "\n",
        "email: hanan.aldarmaki@mbzuai.ac.ae\n",
        "\n",
        "# Linear Discriminant Analysis\n",
        "\n",
        "This is a simple example that demonstrates the steps involved in LDA calculations. \n",
        "\n",
        "## 1. The Data \n",
        "\n",
        "We will create a toy dataset that consists of 2D points and 2 classes (each class has 5 samples). To be consist with the format used in sklean, we will create one data matrix, X, and a vector of labels, Y. We will use the labels 1 and 2 for the two classes. "
      ],
      "metadata": {
        "id": "ZVC6qqL9gbWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#data array\n",
        "X=np.array([[4,1],[2,4],[2,3], [3,6], [4,4],[9,10],[6,8],[9,5], [8,7], [10,8] ])\n",
        "\n",
        "#labels vector\n",
        "Y=np.array([1,1,1,1,1,2,2,2,2,2])\n",
        "\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "8Ofo1bABZd5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following block defines a function that we will use to plot the data. The input is X, Y, and a title for the plot. "
      ],
      "metadata": {
        "id": "dQSULWUAeQeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting function\n",
        "from matplotlib import pyplot as plt\n",
        "label_ids = {1: 'class1', 2: 'class2'}\n",
        "def plot_data(X, Y, title):\n",
        "    ax = plt.subplot(111)\n",
        "    for label,marker,color in zip(\n",
        "        range(1,3),('s', 'o'),('blue', 'red')):\n",
        "\n",
        "        plt.scatter(x=X[Y == label, 0],y=X[Y == label, 1],\n",
        "                    marker=marker,\n",
        "                    color=color,\n",
        "                    alpha=0.5,\n",
        "                    label=label_ids[label])\n",
        "\n",
        "    leg = plt.legend(loc='upper right', fancybox=True)\n",
        "    leg.get_frame().set_alpha(0.5)\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.grid()\n",
        "    plt.tight_layout\n"
      ],
      "metadata": {
        "id": "G49N-no-xwPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data(X,Y, \"Original dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hv4nG1IJyP96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. LDA calculations\n",
        "\n",
        "We now calculate the mean of each class:\n",
        "\n",
        "$ \\mu_j = \\frac{1}{n_j} \\sum_{x_i \\in c_j} x_i$\n",
        "\n",
        "and the total mean:\n",
        "\n",
        "$ \\mu = \\frac{1}{N} \\sum_{j=1}^C n_j\\mu_j$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TmhcqFktqdcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u1=np.mean(X[Y==1], axis=0)\n",
        "u2=np.mean(X[Y==2], axis=0)\n",
        "\n",
        "#total mean:\n",
        "u=(u1+u2)/2\n",
        "\n",
        "print(u)\n"
      ],
      "metadata": {
        "id": "O40_4V_SeCjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Within Class Variance:\n",
        "\n",
        "$S_{Wj} = \\sum_{x \\in c_j}(x-\\mu_j)(x-\\mu_j)^T$\n",
        "\n",
        "$S_W = \\sum_{j=1}^C S_{Wj}$"
      ],
      "metadata": {
        "id": "SOeRX82ufwcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mean-centered data:\n",
        "x1_c=X[Y==1]-u1\n",
        "x2_c=X[Y==2]-u2\n",
        "\n",
        "sw_1=np.outer(x1_c[0],x1_c[0].T)\n",
        "sw_2=np.outer(x2_c[0],x2_c[0].T)\n",
        "\n",
        "for i in range(1,5):\n",
        " sw_1+= np.outer(x1_c[i],x1_c[i])\n",
        " sw_2+= np.outer(x2_c[i],x2_c[i])\n",
        "\n",
        "print(\"Sw using for loop\")\n",
        "print(sw_1)\n",
        "#alternatively, we can calculate it in one go by matrix multiplication:\n",
        "print(\"\\nUsing matrix multiplication:\")\n",
        "print(x1_c.T.dot(x1_c))"
      ],
      "metadata": {
        "id": "Q3c7IHN3enjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sw=sw_1+sw_2\n",
        "print(Sw)"
      ],
      "metadata": {
        "id": "Ty2UWCscnsvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Between Class Variance:\n",
        "\n",
        "$S_{Bj} = (\\mu_j-\\mu)(\\mu_j-\\mu)^T$\n",
        "\n",
        "$S_B = \\sum_{j=1}^C n_j S_{Bj}$"
      ],
      "metadata": {
        "id": "2o80z9APgrsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "s1_b=np.outer((u1-u).T, u1-u )\n",
        "s2_b=np.outer((u2-u).T, u2-u )\n",
        "\n",
        "Sb=5*s1_b+5*s2_b #the scaling makes no difference here (it only scales the eigenvalues)\n",
        "print(Sb)"
      ],
      "metadata": {
        "id": "JtZ9feR3oBOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find projection W\n",
        "\n",
        "Calculate the eignevalues/eigenvectors of $S_W^{-1}S_B$\n"
      ],
      "metadata": {
        "id": "RzHj2iiEhNoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "R=np.linalg.inv(Sw).dot(Sb)\n",
        "evals,V = np.linalg.eig(R)\n",
        "\n",
        "for i in range(len(evals)):\n",
        "    eigvec_sc = V[:,i].reshape(2,1)   \n",
        "    print('\\nEigenvector {}: \\n{}'.format(i+1, eigvec_sc.real))\n",
        "    print('Eigenvalue {:}: {:.2e}'.format(i+1, evals[i].real))\n",
        "\n"
      ],
      "metadata": {
        "id": "TeLeUAIFq960"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will keep only 1 eigenvector, which has the largest eigenvalue. This will be our projection W, which we will use to project the points into a line. "
      ],
      "metadata": {
        "id": "sZhalJ1PhjKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transform\n",
        "X_lda= X.dot(np.array( [0.91955932 , 0.39295122 ]).T)\n",
        "\n",
        "plot_data(X,Y, \"Original dataset\")\n",
        "plt.plot([0, 0.91955932*11], [0 , 0.39295122*11 ] )\n",
        "plt.show()\n",
        "\n",
        "#expand to 2D for plotting, the y values are all set to 0\n",
        "X_lda_plot = np.vstack([X_lda, np.zeros(X_lda.shape[0])]).T\n",
        "plot_data(X_lda_plot, Y, \"LDA projection\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bNPHC8o1uS-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LDA function from sklearn\n",
        "\n",
        "The following block shows the steps for using sklearn's implementation of LDA. Note that the details of the implementation in sklearn are slightly different, so the results will appear different, but the plots will be the same (relative positions). "
      ],
      "metadata": {
        "id": "X70sw9vllDki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "clf = LinearDiscriminantAnalysis()\n",
        "clf.fit(X, Y)\n",
        "\n",
        "#transform the data\n",
        "X_lda_sk = clf.transform(X)\n",
        "X_lda_sk = np.vstack([X_lda_sk[:,0], np.zeros(X_lda.shape[0])]).T\n",
        "\n",
        "plot_data(X_lda_sk, Y, title='Default LDA via scikit-learn')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MgpVyGfJtN6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Principal Component Analysis\n",
        "\n",
        "We will now implement PCA using the same dataset. PCA is unsupervised, so we will not use the class lables, Y, except for plotting. \n",
        "\n",
        "First we will calculate the total mean of the data and subtract it from all data points to calculate the covariance\n",
        "\n",
        "$\\mu = \\frac{1}{n}\\sum_{i=1}^Nx_i$\n",
        "\n",
        "$S=\\frac{1}{N}\\sum_{i=1}^N(x_i-\\mu)(x_i-\\mu)^T$"
      ],
      "metadata": {
        "id": "ZUvq-NP81XQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u=np.mean(X, axis=0)\n",
        "X_c=X-u\n",
        "X=X.T #transpose the matrix to get 2x10 matrix (as in slides)\n",
        "S=X.dot(X.T)"
      ],
      "metadata": {
        "id": "2J7xjYpz2OBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we find the eigenvalues/eigenvectos of S, and sort them. We will use the first principal component for projection"
      ],
      "metadata": {
        "id": "-2aQ1oNa2p2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "evals,V = np.linalg.eig(S)\n",
        "print(V)\n",
        "for i in range(len(evals)):\n",
        "    eigvec_sc = V[:,i].reshape(2,1)   \n",
        "    print('\\nEigenvector {}: \\n{}'.format(i+1, eigvec_sc.real))\n",
        "    print('Eigenvalue {:}: {:.2e}'.format(i+1, evals[i].real))\n",
        "\n",
        "p_var=evals[0]/(evals[0]+evals[1])\n",
        "print(\"\\n percentage of Variance explained by first eigenvector:\", p_var*100,\"%\")"
      ],
      "metadata": {
        "id": "zKk64Zx62xTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transform\n",
        "W=np.array( [0.72171413 , 0.69219124 ])\n",
        "X_pca= W.T.dot(X)\n",
        "\n",
        "plot_data(X.T,Y, \"Original dataset\")\n",
        "plt.plot([0, 0.72171413*11], [0 , 0.69219124*11 ] )\n",
        "plt.show()\n",
        "\n",
        "#expand to 2D for plotting, the y values are all set to 0\n",
        "X_pca_plot = np.vstack([X_pca.T, np.zeros(X_pca.T.shape[0])]).T\n",
        "plot_data(X_pca_plot, Y, \"LDA projection\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fI5M4N273WUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconstructing the data\n",
        "\n",
        "In the previous example, we used only the first principal component to reduce the dimensionality of the data. We can reconstruct the original data perfectly if we keep both components, as shown below. If we discard the second component, we can still reconstruct X, but with  loss. Since we drop the second PC, the variance in that direction will be lost completely. "
      ],
      "metadata": {
        "id": "09mPh_tgCNYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_full = np.array( [[0.72171413 , 0.69219124 ], [-0.69219124, 0.72171413]]).T\n",
        "X_pca_full = W_full.T.dot(X)\n",
        "print(X_pca_full.shape, X.shape)\n",
        "X_reconst = W_full.dot(X_pca_full)\n",
        "print(\"Original X:\")\n",
        "print(X)\n",
        "\n",
        "print(\"\\n reconstructed X from full pca dimensiolns\")\n",
        "print(X_reconst)\n",
        "\n",
        "\n",
        "print(\"\\n reconstructed X from first PC\")\n",
        "X_reconst = (np.expand_dims(W, axis=1).dot(np.expand_dims(X_pca, axis=1).T))\n",
        "print(X_reconst)\n",
        "\n"
      ],
      "metadata": {
        "id": "X2Mor8BjCMoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data(X.T,Y, \"Original/new dataset\")\n",
        "#X_reconst=X_reconst.T\n",
        "for label,marker,color in zip(\n",
        "        range(1,3),('s', 'o'),('green', 'black')):\n",
        "\n",
        "        plt.scatter(x=X_reconst.T[Y == label, 0],y=X_reconst.T[Y == label, 1],\n",
        "                    marker=marker,\n",
        "                    color=color,\n",
        "                    alpha=0.5,\n",
        "                    label=label_ids[label])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sNB3bJLMFGad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Semantic Analysis\n",
        "\n",
        "Here we construct the example shown in the sldies. We have a term-document matrix. We have 9 documents, and 6 words. We show the cosine similarity before and after applying SVD for dimensionality reduction. \n"
      ],
      "metadata": {
        "id": "zN2Ts-GI6zJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TD=np.array([[0,1,1,1,0,1,0,0,1],\n",
        "             [1,0,0,1,1,1,1,1,1],\n",
        "             [1,0,0,0,0,0,1,0,0],\n",
        "             [0,1,0,1,0,0,1,0,1],\n",
        "             [0,0,1,0,1,1,0,0,0],\n",
        "             [0,0,0,0,0,1,0,1,0]])\n",
        "\n",
        "print(\"Before SVD:\")\n",
        "\n",
        "cosine=TD[:,0].dot(TD[:, 1])/(norm(TD[:,0])*norm(TD[:,1]))\n",
        "print(\"cosine similarity of first two documents :\", cosine)\n",
        "\n",
        "cosine=TD[2,:].dot(TD[3,:])/(norm(TD[2,:])*norm(TD[3,:]))\n",
        "print(\"cosine similarity of words at index 2 and 3 (funny, hilarious) :\", cosine)\n",
        "\n",
        "cosine=TD[2,:].dot(TD[4,:])/(norm(TD[2,:])*norm(TD[4,:]))\n",
        "print(\"cosine similarity of words at index 2 and 4 (funny, sad) :\", cosine)"
      ],
      "metadata": {
        "id": "32iDtwcgYTo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U,L,V=np.linalg.svd(TD)\n",
        "from numpy.linalg import norm\n",
        "print(\"After SVD and reducing dimensionality to 2:\")\n",
        "\n",
        "w1=V[0:2,0]\n",
        "w2=V[0:2,1]\n",
        "cosine=w1.dot(w2)/(norm(w1)*norm(w2))\n",
        "print(\"cosine similarity between first two documents:\", cosine)\n",
        "\n",
        "w1=U[2,0:2]\n",
        "w2=U[3,0:2]\n",
        "cosine=w1.dot(w2)/(norm(w1)*norm(w2))\n",
        "print(\"cosine similarity between the words (funny, hilarious):\", cosine)\n",
        "\n",
        "w1=U[2,0:2]\n",
        "w2=U[4,0:2]\n",
        "cosine=w1.dot(w2)/(norm(w1)*norm(w2))\n",
        "print(\"cosine similarity between the words (funny, sad):\", cosine)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wi7248PgZOCW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
